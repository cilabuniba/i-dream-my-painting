seed: 42
model:
  id: llava-hf/llava-v1.6-vicuna-7b-hf
  fine_tuning_type: quantized # can be 'full', 'lora', 'qlora'
  lora_config_partial:
    _partial_: true
    _target_: peft.LoraConfig
    r: 16
    lora_alpha: 16
    lora_dropout: 5.e-2
    init_lora_weights: gaussian
alpha_schedulers:
  train:
    _target_: inpainting.models.image_to_text.schedulers.ConstantAlphaScheduler
    alpha: 1.0
  val:
    _target_: inpainting.models.image_to_text.schedulers.ConstantAlphaScheduler
    alpha: 1.0
datasets:
  train:
    _target_: inpainting.data.datasets.LlavaDataset
    data_dir: data/mm_inp_dataset
    max_concepts: 5
    remove_intersections: false
    generator: null
    shuffle_concepts: true
    masked_area_threshold: 0.65
    return_entity_PILs: false
    only_gray_concept: true
    split: train
  val:
    _target_: inpainting.data.datasets.LlavaDataset
    data_dir: ${datasets.train.data_dir}
    max_concepts: ${datasets.train.max_concepts}
    remove_intersections: ${datasets.train.remove_intersections}
    generator:
      _target_: torch.Generator
    shuffle_concepts: ${datasets.train.shuffle_concepts}
    masked_area_threshold: ${datasets.train.masked_area_threshold}
    return_entity_PILs: true
    only_gray_concept: true
    split: test
batch_samplers:
  train_partial:
    _partial_: true
    _target_: inpainting.models.image_to_text.samplers.AlphaScheduleBatchSampler
    max_concepts: ${datasets.train.max_concepts}
    batch_size: 8
    shuffle: true
    drop_last: false
    generator: null
  val_partial:
    _partial_: true
    _target_: inpainting.models.image_to_text.samplers.AlphaScheduleBatchSampler
    max_concepts: ${datasets.train.max_concepts}
    batch_size: 8
    shuffle: false
    drop_last: false
    generator: null
dataloaders:
  train_partial:
    _partial_: true
    _target_: torch.utils.data.DataLoader
    num_workers: 8
  val_partial:
    _partial_: true
    _target_: torch.utils.data.DataLoader
    num_workers: 8
accelerator:
  _target_: accelerate.Accelerator
  gradient_accumulation_steps: 1
  mixed_precision: bf16
  log_with: wandb
  project_config:
    _target_: accelerate.utils.ProjectConfiguration
    project_dir: models/llava/1mask
    logging_dir: ${accelerator.project_config.project_dir}/logs
optimizer_partial:
  _partial_: true
  _target_: torch.optim.AdamW
  lr: 2.e-4
  betas: [0.9, 0.999]
  weight_decay: 0.01
  eps: 1.e-8
scheduler:
  name: constant_with_warmup
  warmup_ratio: 0.01
checkpointing_steps: null
max_grad_norm: 0.5
max_length: 512
max_train_steps: null
max_val_instances: 5000
num_train_epochs: 1
num_train_images_to_log: 100
num_val_images_to_log: 100
only_val: true
validate_every_n_epochs: 1
resume_from_checkpoint: null
init_trackers_kwargs:
  group: ImageToText
  tags: [prompt, test]
